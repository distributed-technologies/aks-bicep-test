apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
spec:
  releaseName: rook-ceph-cluster
  chart:
    spec:
      chart: rook-ceph-cluster
      sourceRef:
        kind: HelmRepository
        name: rook-ceph
        namespace: flux-system
  interval: 10m
  install:
    remediation:
      retries: 3
  upgrade:
    remediation:
      retries: 3
  values:
    image:
      tag: v1.7.2
  cephClusterSpec:
    mon:
      volumeClaimTemplate:
        spec:
          storageClassName: managed-premium
          resources:
            requests:
              storage: 10Gi
    storage:
      storageClassDeviceSets:
        - name: set1
          # The number of OSDs to create from this device set
          count: 3
          # IMPORTANT: If volumes specified by the storageClassName are not portable across nodes
          # this needs to be set to false. For example, if using the local storage provisioner
          # this should be false.
          portable: false
          # Since the OSDs could end up on any node, an effort needs to be made to spread the OSDs
          # across nodes as much as possible. Unfortunately the pod anti-affinity breaks down
          # as soon as you have more than one OSD per node. The topology spread constraints will
          # give us an even spread on K8s 1.18 or newer.
          placement:
            topologySpreadConstraints:
              - maxSkew: 1
                topologyKey: kubernetes.io/hostname
                whenUnsatisfiable: ScheduleAnyway
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - rook-ceph-osd
            tolerations:
              - key: storage-node
                operator: Exists
          preparePlacement:
            tolerations:
              - key: storage-node
                operator: Exists
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                  - matchExpressions:
                      - key: agentpool
                        operator: In
                        values:
                          - npceph
            topologySpreadConstraints:
              - maxSkew: 1
                # IMPORTANT: If you don't have zone labels, change this to another key such as kubernetes.io/hostname
                topologyKey: topology.kubernetes.io/zone
                whenUnsatisfiable: DoNotSchedule
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - rook-ceph-osd-prepare
          resources:
            limits:
              cpu: "500m"
              memory: "4Gi"
            requests:
              cpu: "500m"
              memory: "2Gi"
          volumeClaimTemplates:
            - metadata:
                name: data
              spec:
                resources:
                  requests:
                    storage: 100Gi
                storageClassName: managed-premium
                volumeMode: Block
                accessModes:
                  - ReadWriteOnce
